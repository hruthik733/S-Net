训练集数量：700，训练集索引：[640, 357, 596, 833, 896, 206, 422, 228, 818, 840, 953, 871, 240, 156, 875, 470, 693, 468, 620, 272, 870, 917, 146, 849, 861, 249, 102, 980, 389, 947, 628, 6, 729, 882, 293, 125, 54, 46, 226, 298, 39, 688, 176, 724, 258, 933, 216, 937, 988, 673, 409, 768, 329, 413, 999, 192, 641, 945, 745, 562, 559, 838, 274, 654, 126, 478, 61, 383, 182, 938, 437, 285, 897, 79, 740, 428, 59, 28, 358, 668, 885, 517, 384, 545, 702, 25, 317, 372, 118, 677, 766, 719, 209, 778, 294, 963, 627, 18, 229, 172, 887, 395, 412, 962, 325, 499, 305, 551, 801, 32, 951, 706, 179, 772, 820, 844, 230, 767, 506, 932, 103, 723, 734, 131, 408, 926, 900, 771, 687, 929, 869, 476, 448, 525, 161, 392, 13, 568, 14, 685, 588, 153, 915, 138, 22, 353, 758, 273, 283, 821, 91, 977, 217, 696, 813, 57, 359, 436, 306, 872, 344, 188, 202, 82, 186, 753, 149, 792, 73, 603, 812, 935, 50, 284, 326, 139, 550, 728, 528, 851, 836, 405, 923, 939, 134, 74, 410, 663, 312, 449, 251, 388, 346, 681, 318, 135, 35, 984, 299, 336, 705, 168, 608, 8, 416, 335, 837, 183, 714, 750, 267, 301, 700, 108, 864, 546, 113, 564, 781, 618, 890, 275, 201, 684, 122, 190, 731, 894, 609, 40, 341, 822, 703, 492, 783, 429, 425, 579, 710, 481, 123, 532, 952, 159, 142, 239, 611, 581, 867, 441, 259, 600, 904, 136, 345, 981, 878, 960, 291, 537, 974, 774, 464, 455, 807, 360, 424, 407, 966, 733, 391, 150, 453, 496, 707, 484, 969, 352, 447, 678, 931, 20, 541, 47, 614, 735, 973, 811, 921, 732, 650, 558, 119, 552, 174, 269, 712, 565, 148, 522, 934, 222, 338, 328, 672, 157, 375, 540, 991, 834, 289, 746, 606, 129, 45, 630, 515, 633, 402, 874, 403, 460, 187, 194, 72, 911, 965, 616, 90, 469, 624, 442, 433, 504, 692, 457, 967, 271, 415, 121, 376, 995, 787, 302, 151, 533, 827, 323, 141, 296, 400, 909, 755, 850, 862, 386, 219, 303, 334, 512, 109, 87, 823, 371, 877, 597, 279, 218, 955, 736, 220, 928, 177, 80, 879, 863, 669, 397, 903, 832, 716, 58, 1, 629, 263, 848, 435, 421, 936, 430, 956, 761, 95, 12, 83, 105, 494, 356, 682, 193, 943, 252, 450, 757, 586, 244, 805, 307, 140, 689, 41, 292, 665, 744, 686, 261, 115, 626, 578, 776, 664, 406, 507, 300, 280, 16, 333, 743, 366, 178, 97, 571, 574, 666, 625, 925, 797, 543, 511, 340, 205, 803, 601, 704, 950, 53, 379, 961, 501, 699, 842, 538, 444, 676, 236, 808, 946, 56, 130, 865, 990, 789, 817, 489, 361, 986, 432, 852, 509, 330, 764, 780, 916, 255, 348, 351, 277, 941, 320, 655, 43, 373, 199, 548, 907, 959, 445, 117, 621, 708, 927, 553, 295, 972, 674, 465, 31, 314, 711, 29, 440, 78, 238, 924, 652, 213, 742, 94, 245, 287, 631, 694, 495, 855, 235, 37, 443, 610, 475, 594, 779, 34, 602, 68, 593, 730, 561, 426, 784, 500, 975, 721, 920, 241, 868, 968, 635, 563, 86, 473, 531, 717, 242, 636, 364, 996, 555, 998, 411, 107, 114, 816, 964, 695, 85, 659, 647, 942, 165, 493, 819, 308, 55, 901, 985, 304, 390, 93, 438, 175, 795, 454, 71, 2, 70, 679, 639, 587, 535, 741, 324, 590, 254, 983, 7, 434, 106, 247, 508, 958, 155, 713, 521, 265, 788, 439, 599, 786, 27, 211, 185, 572, 567, 451, 463, 477, 81, 763, 5, 88, 311, 895, 92, 970, 315, 144, 760, 488, 160, 198, 989, 310, 617, 316, 754, 698, 203, 940, 288, 539, 615, 278, 98, 319, 994, 233, 367, 152, 505, 756, 912, 566, 42, 831, 765, 715, 891, 77, 49, 648, 232, 523, 554, 876, 518, 401, 207, 189, 697, 420, 598, 799, 36, 490, 270, 824, 158, 638, 110, 196, 486, 17, 268, 24, 380, 930, 908, 181, 264, 814, 215, 456, 886, 569, 309, 398, 577, 15, 632, 377, 580, 163, 725, 957, 607, 544, 892, 643, 414, 971]
验证集数量：102，验证集索引：[60, 556, 347, 342, 576, 69, 845, 683, 825, 843, 404, 796, 570, 592, 634, 738, 519, 785, 30, 143, 479, 922, 847, 137, 524, 337, 290, 23, 200, 853, 116, 589, 458, 276, 11, 644, 658, 363, 171, 826, 884, 350, 208, 164, 798, 997, 10, 513, 858, 394, 902, 748, 612, 782, 918, 147, 667, 651, 573, 859, 96, 48, 846, 534, 913, 530, 806, 327, 339, 365, 180, 881, 132, 485, 431, 257, 948, 184, 839, 370, 234, 378, 982, 191, 830, 169, 396, 459, 752, 623, 910, 313, 649, 266, 52, 661, 502, 898, 51, 418, 75, 474]
测试集数量：198，测试集索引：[417, 204, 660, 173, 613, 802, 595, 225, 979, 919, 804, 223, 224, 462, 622, 214, 978, 726, 349, 737, 860, 297, 536, 976, 653, 89, 162, 873, 637, 467, 549, 993, 709, 127, 880, 645, 516, 44, 369, 769, 906, 246, 461, 773, 243, 791, 19, 619, 145, 419, 751, 38, 701, 883, 899, 889, 759, 720, 26, 657, 33, 718, 427, 354, 582, 446, 605, 170, 387, 282, 212, 762, 260, 250, 253, 331, 382, 76, 680, 498, 0, 133, 101, 286, 949, 167, 747, 646, 111, 510, 362, 670, 503, 124, 385, 662, 332, 84, 800, 514, 905, 526, 992, 841, 393, 237, 381, 835, 671, 944, 604, 4, 112, 64, 154, 399, 527, 491, 21, 856, 893, 547, 227, 197, 656, 374, 100, 987, 466, 794, 62, 954, 810, 3, 770, 480, 262, 691, 452, 829, 210, 727, 423, 471, 248, 591, 63, 221, 585, 828, 749, 642, 483, 343, 281, 195, 321, 584, 854, 777, 472, 542, 166, 520, 529, 256, 560, 809, 497, 104, 583, 368, 99, 888, 65, 128, 675, 857, 482, 355, 739, 690, 775, 557, 120, 815, 722, 575, 914, 322, 9, 793, 866, 67, 231, 790, 487, 66]
time             2025-04-11 11:38:52
id               S_Net
data             Kvasir_png_224
n_splits         5
save_path        ./save_model/
epochs           120
early_stop       150
batch_size       16
lr               0.001
momentum         0.9
weight_decay     0.0001
--------------------------------------------------
Network Architecture of Model S_Net:
S_Net(
  (encoder1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (encoder2): Sequential(
    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (GLFFs): GLFFs(
    (layers): ModuleList(
      (0): GLFF(
        (blocks): ModuleList(
          (0): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.0)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.014285714365541935)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.014285714365541935, inplace=False)
            )
          )
        )
        (conv_layer): MSFF(
          (conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
          (conv0_1): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=64)
          (conv0_2): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=64)
          (conv1_1): Conv2d(64, 64, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=64)
          (conv1_2): Conv2d(64, 64, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=64)
          (conv2_1): Conv2d(64, 64, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=64)
          (conv2_2): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64)
          (conv12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop): timm.DropPath(0)
        )
      )
      (1): GLFF(
        (blocks): ModuleList(
          (0): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.02857142873108387)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.02857142873108387, inplace=False)
            )
          )
          (1): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.04285714402794838)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.04285714402794838, inplace=False)
            )
          )
        )
        (conv_layer): MSFF(
          (conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
          (conv0_1): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=64)
          (conv0_2): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=64)
          (conv1_1): Conv2d(64, 64, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=64)
          (conv1_2): Conv2d(64, 64, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=64)
          (conv2_1): Conv2d(64, 64, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=64)
          (conv2_2): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64)
          (conv12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop): timm.DropPath(0)
        )
      )
      (2): GLFF(
        (blocks): ModuleList(
          (0): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.05714285746216774)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.05714285746216774, inplace=False)
            )
          )
          (1): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.0714285746216774)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.0714285746216774, inplace=False)
            )
          )
        )
        (conv_layer): MSFF(
          (conv0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
          (conv0_1): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), groups=64)
          (conv0_2): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=64)
          (conv1_1): Conv2d(64, 64, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=64)
          (conv1_2): Conv2d(64, 64, kernel_size=(11, 1), stride=(1, 1), padding=(5, 0), groups=64)
          (conv2_1): Conv2d(64, 64, kernel_size=(1, 21), stride=(1, 1), padding=(0, 10), groups=64)
          (conv2_2): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64)
          (conv12): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop): timm.DropPath(0)
        )
      )
      (3): GLFF(
        (blocks): ModuleList(
          (0): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.08571428805589676)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.08571428805589676, inplace=False)
            )
          )
          (1): VSS_ConvG(
            (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              (in_proj): Linear(in_features=64, out_features=256, bias=False)
              (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (act): SiLU()
              (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(in_features=128, out_features=64, bias=False)
            )
            (drop_path): timm.DropPath(0.10000000149011612)
            (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (mlp): ConvolutionalGLU(
              (fc1): Linear(in_features=64, out_features=84, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=42)
              )
              (act): SiLU()
              (fc2): Linear(in_features=42, out_features=64, bias=True)
              (drop): Dropout(p=0.10000000149011612, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder1): Sequential(
    (0): conv_block(
      (conv): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (1): res_conv_block2(
      (conv): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU()
    )
    (2): res_conv_block2(
      (conv): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU()
    )
  )
  (decoder2): Sequential(
    (0): conv_block(
      (conv): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (1): res_conv_block2(
      (conv): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU()
    )
    (2): res_conv_block2(
      (conv): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU()
    )
  )
  (final): Sequential(
    (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
--------------------------------------------------
Number of trainable parameters 1522306 in Model S_Net
Epoch:0/120, lr:0.001
Epoch:0，train_loss:1.232415407725743
Epoch:0, val_loss:1.2304194479012023，val_iou:0.2831563101512181，val_dice:0.40003012290963863
=> saved best model——2025.04.11-11:40:19
Epoch:1/120, lr:0.001
Epoch:1，train_loss:1.2211456762041364
Epoch:1, val_loss:1.2622809491905511，val_iou:0.28216493034811496，val_dice:0.3992777435607826
Epoch:2/120, lr:0.001
Epoch:2，train_loss:1.2029904222488403
Epoch:2, val_loss:1.1814781003722958，val_iou:0.2804199943276905，val_dice:0.396530151587767
Epoch:3/120, lr:0.001
Epoch:3，train_loss:1.1855455916268485
Epoch:3, val_loss:1.304100891246515，val_iou:0.2785257985991494，val_dice:0.3964682901678521
Epoch:4/120, lr:0.001
Epoch:4，train_loss:1.1736833204541888
Epoch:4, val_loss:1.4149219379121183，val_iou:0.28812057621216364，val_dice:0.4064176516983501
=> saved best model——2025.04.11-11:46:15
Epoch:5/120, lr:0.001
Epoch:5，train_loss:1.1671214621407644
Epoch:5, val_loss:1.1741218818169015，val_iou:0.2630738460489415，val_dice:0.37790661747853804
Epoch:6/120, lr:0.001
Epoch:6，train_loss:1.140966018949236
Epoch:6, val_loss:1.1370550189532487，val_iou:0.30027797491370994，val_dice:0.41914501894706313
=> saved best model——2025.04.11-11:49:13
Epoch:7/120, lr:0.001
Epoch:7，train_loss:1.1357193470001221
Epoch:7, val_loss:1.2403988940458672，val_iou:0.30513797484098015，val_dice:0.4239239727260823
=> saved best model——2025.04.11-11:50:42
Epoch:8/120, lr:0.001
Epoch:8，train_loss:1.1170028053011214
Epoch:8, val_loss:1.096923369402979，val_iou:0.2657238734983759，val_dice:0.3805379135301504
Epoch:9/120, lr:0.001
Epoch:9，train_loss:1.1719592605318343
Epoch:9, val_loss:1.1781160334746044，val_iou:0.2998294151052316，val_dice:0.4202483274349613
Epoch:10/120, lr:0.001
Epoch:10，train_loss:1.1413720798492433
Epoch:10, val_loss:1.1720826310269974，val_iou:0.2961035893577284，val_dice:0.41376442441032235
Epoch:11/120, lr:0.001
Epoch:11，train_loss:1.1282211290087019
Epoch:11, val_loss:1.0700170675329133，val_iou:0.2758822863900123，val_dice:0.3929480521795671
Epoch:12/120, lr:0.001
Epoch:12，train_loss:1.1189836992536273
Epoch:12, val_loss:1.038891165864234，val_iou:0.3195320779013166，val_dice:0.43876925793561344
=> saved best model——2025.04.11-11:58:06
Epoch:13/120, lr:0.001
Epoch:13，train_loss:1.102151654788426
Epoch:13, val_loss:1.117681762751411，val_iou:0.2197915098222229，val_dice:0.31447265571468325
Epoch:14/120, lr:0.001
Epoch:14，train_loss:1.1226109252657208
Epoch:14, val_loss:1.474349256531865，val_iou:0.29301874413501655，val_dice:0.4097976878525688
Epoch:15/120, lr:0.001
Epoch:15，train_loss:1.1160240043912615
Epoch:15, val_loss:1.0444837460915248，val_iou:0.28759563111466396，val_dice:0.3944108950534558
Epoch:16/120, lr:0.001
Epoch:16，train_loss:1.0937182194846018
Epoch:16, val_loss:1.0150687872779136，val_iou:0.32532670974919736，val_dice:0.44441830407653016
=> saved best model——2025.04.11-12:04:02
Epoch:17/120, lr:0.001
Epoch:17，train_loss:1.1364833613804408
Epoch:17, val_loss:0.9498323184602401，val_iou:0.3318466238939526，val_dice:0.4483842435649389
=> saved best model——2025.04.11-12:05:30
Epoch:18/120, lr:0.001
Epoch:18，train_loss:1.1221328708103724
Epoch:18, val_loss:1.091477092866804，val_iou:0.29318254564079016，val_dice:0.40350635447793837
Epoch:19/120, lr:0.001
Epoch:19，train_loss:1.1009961482456752
Epoch:19, val_loss:1.0503318320010222，val_iou:0.3100648818060354，val_dice:0.43286875543460424
Epoch:20/120, lr:0.001
Epoch:20，train_loss:1.0680100876944405
Epoch:20, val_loss:1.0668793162878822，val_iou:0.32463327193546776，val_dice:0.4501270900211778
Epoch:21/120, lr:0.001
Epoch:21，train_loss:1.0489667279379709
Epoch:21, val_loss:1.2292275602618854，val_iou:0.3114401268568075，val_dice:0.4346773789280503
Epoch:22/120, lr:0.001
Epoch:22，train_loss:1.0935325472695487
Epoch:22, val_loss:0.9422869423733038，val_iou:0.3775094516884709，val_dice:0.49960414185357704
=> saved best model——2025.04.11-12:12:54
Epoch:23/120, lr:0.001
Epoch:23，train_loss:1.0584571089063373
Epoch:23, val_loss:1.2371427764495213，val_iou:0.3089741886736107，val_dice:0.4303134128243592
Epoch:24/120, lr:0.001
Epoch:24，train_loss:1.0963659872327531
Epoch:24, val_loss:0.9809420932741726，val_iou:0.3249764778716014，val_dice:0.4309445610555141
Epoch:25/120, lr:0.001
Epoch:25，train_loss:1.0788212163107735
Epoch:25, val_loss:1.0134896595104068，val_iou:0.34413696077106715，val_dice:0.47106731866438717
Epoch:26/120, lr:0.001
Epoch:26，train_loss:1.061674827166966
Epoch:26, val_loss:0.9675682949669221，val_iou:0.30331622298973754，val_dice:0.389593663882159
Epoch:27/120, lr:0.001
Epoch:27，train_loss:1.0475490965162004
Epoch:27, val_loss:0.9043372504267038，val_iou:0.3642703449455103，val_dice:0.4821552974600298
Epoch:28/120, lr:0.001
Epoch:28，train_loss:1.08083083152771
Epoch:28, val_loss:1.1261434845772444，val_iou:0.3236835828295782，val_dice:0.44886209834482954
Epoch:29/120, lr:0.001
Epoch:29，train_loss:1.0448784705570766
Epoch:29, val_loss:0.8859793210730833，val_iou:0.39479759889245936，val_dice:0.4961402455955685
=> saved best model——2025.04.11-12:23:15
Epoch:30/120, lr:0.001
Epoch:30，train_loss:1.0422116065025329
Epoch:30, val_loss:0.8946240133806771，val_iou:0.3582400492418497，val_dice:0.4653732871967822
Epoch:31/120, lr:0.001
Epoch:31，train_loss:1.035687471798488
Epoch:31, val_loss:1.1748506082039254，val_iou:0.31921551813353344，val_dice:0.44514987597046457
Epoch:32/120, lr:0.001
Epoch:32，train_loss:1.0425359507969447
Epoch:32, val_loss:0.9195226725994372，val_iou:0.3710312809792247，val_dice:0.4958533457520925
Epoch:33/120, lr:0.001
Epoch:33，train_loss:1.0547539721216475
Epoch:33, val_loss:0.9649317340523589，val_iou:0.3492639506274188，val_dice:0.4736616622485131
Epoch:34/120, lr:0.001
Epoch:34，train_loss:1.0376162215641567
Epoch:34, val_loss:0.9103406556681091，val_iou:0.38453041173842284，val_dice:0.5141317140431404
Epoch:35/120, lr:0.001
Epoch:35，train_loss:1.0232674516950335
Epoch:35, val_loss:0.8649737995629218，val_iou:0.3777545970029683，val_dice:0.4896892651544189
Epoch:36/120, lr:0.001
Epoch:36，train_loss:1.0277513647079468
Epoch 00037: reducing learning rate of group 0 to 5.0000e-04.
Epoch:36, val_loss:1.0331839978987096，val_iou:0.3559262467855626，val_dice:0.4782997849560232
Epoch:37/120, lr:0.0005
Epoch:37，train_loss:1.0126305845805577
Epoch:37, val_loss:0.7984680914703537，val_iou:0.4378204898256371，val_dice:0.5546238470108046
=> saved best model——2025.04.11-12:35:06
Epoch:38/120, lr:0.0005
Epoch:38，train_loss:0.9759218883514404
Epoch:38, val_loss:0.817042375312132，val_iou:0.4183802705453917，val_dice:0.5492408025399712
Epoch:39/120, lr:0.0005
Epoch:39，train_loss:0.980431787286486
Epoch:39, val_loss:0.894237553079923，val_iou:0.3986560593074438，val_dice:0.5236640373603025
Epoch:40/120, lr:0.0005
Epoch:40，train_loss:0.969787518296923
Epoch:40, val_loss:0.8339425478787983，val_iou:0.4137262257625572，val_dice:0.5434418133402155
Epoch:41/120, lr:0.0005
Epoch:41，train_loss:1.0009592076710292
Epoch:41, val_loss:0.9049845095358643，val_iou:0.40499653211736825，val_dice:0.5326057999993113
Epoch:42/120, lr:0.0005
Epoch:42，train_loss:0.9774276433672224
Epoch:42, val_loss:0.8880441814076667，val_iou:0.41500445087421767，val_dice:0.5445652815786016
Epoch:43/120, lr:0.0005
Epoch:43，train_loss:0.9547136953898838
Epoch:43, val_loss:0.821667928175599，val_iou:0.4299377750493769，val_dice:0.5598273862852678
Epoch:44/120, lr:0.0005
Epoch:44，train_loss:0.9304183455875942
Epoch:44, val_loss:0.75613636669575，val_iou:0.47023731253912454，val_dice:0.5945282158052676
=> saved best model——2025.04.11-12:45:28
Epoch:45/120, lr:0.0005
Epoch:45，train_loss:0.9517578499657767
Epoch:45, val_loss:0.7641056856396151，val_iou:0.4540144736660798，val_dice:0.5745268311489766
Epoch:46/120, lr:0.0005
Epoch:46，train_loss:0.9635808999197824
Epoch:46, val_loss:0.8378940456930328，val_iou:0.4287038815999194，val_dice:0.5598015882243605
Epoch:47/120, lr:0.0005
Epoch:47，train_loss:0.9123630496433803
Epoch:47, val_loss:0.7946226452203358，val_iou:0.43334194883596167，val_dice:0.5598459988204787
Epoch:48/120, lr:0.0005
Epoch:48，train_loss:0.9185579613276891
Epoch:48, val_loss:0.7673082601498155，val_iou:0.4481543724220498，val_dice:0.5760316052557934
Epoch:49/120, lr:0.0005
Epoch:49，train_loss:0.9461875806535993
Epoch:49, val_loss:0.7528255722101997，val_iou:0.4703746870396243，val_dice:0.5871740985359639
=> saved best model——2025.04.11-12:52:52
Epoch:50/120, lr:0.0005
Epoch:50，train_loss:0.906168018068586
Epoch:50, val_loss:0.6900505328587457，val_iou:0.48683129418901533，val_dice:0.6029510008074791
=> saved best model——2025.04.11-12:54:21
Epoch:51/120, lr:0.0005
Epoch:51，train_loss:0.9319657598223005
Epoch:51, val_loss:0.8679566284020742，val_iou:0.4028039332216256，val_dice:0.5310245387665926
Epoch:52/120, lr:0.0005
Epoch:52，train_loss:0.9325371299471173
Epoch:52, val_loss:0.6589345074459618，val_iou:0.5189203697123722，val_dice:0.6282586414130112
=> saved best model——2025.04.11-12:57:18
Epoch:53/120, lr:0.0005
Epoch:53，train_loss:0.9272201722008842
Epoch:53, val_loss:0.9064290510380969，val_iou:0.4111894573169011，val_dice:0.5372451770689614
Epoch:54/120, lr:0.0005
Epoch:54，train_loss:0.9223424618584769
Epoch:54, val_loss:0.6649525025311638，val_iou:0.5194007022141113，val_dice:0.6336609465611257
=> saved best model——2025.04.11-13:00:15
Epoch:55/120, lr:0.0005
Epoch:55，train_loss:0.9076749631336757
Epoch:55, val_loss:0.7266739668215022，val_iou:0.48954891402885564，val_dice:0.605685442228206
Epoch:56/120, lr:0.0005
Epoch:56，train_loss:0.8996091106959752
Epoch:56, val_loss:0.6585095784827775，val_iou:0.5179781699494338，val_dice:0.6219610115785996
Epoch:57/120, lr:0.0005
Epoch:57，train_loss:0.8848053394045149
Epoch:57, val_loss:0.7743726590392637，val_iou:0.4620311412032889，val_dice:0.5896320265570509
Epoch:58/120, lr:0.0005
Epoch:58，train_loss:0.8581647600446428
Epoch:58, val_loss:0.7494715316330686，val_iou:0.48469882734206227，val_dice:0.6078236269115884
Epoch:59/120, lr:0.0005
Epoch:59，train_loss:0.9039495083263942
Epoch:59, val_loss:0.772555788796322，val_iou:0.4738217441302884，val_dice:0.5963682119779474
Epoch:60/120, lr:0.0005
Epoch:60，train_loss:0.8696536139079503
Epoch:60, val_loss:0.8735956853511286，val_iou:0.4163273045300872，val_dice:0.5425148142314812
Epoch:61/120, lr:0.0005
Epoch:61，train_loss:0.8730389751706804
Epoch:61, val_loss:0.6212288286609977，val_iou:0.5426373706089528，val_dice:0.656919526214171
=> saved best model——2025.04.11-13:10:36
Epoch:62/120, lr:0.0005
Epoch:62，train_loss:0.8549361119951521
Epoch:62, val_loss:0.610958901544412，val_iou:0.5423804870328628，val_dice:0.6605295472214894
Epoch:63/120, lr:0.0005
Epoch:63，train_loss:0.8693107414245606
Epoch:63, val_loss:0.6889255531132221，val_iou:0.515277245508227，val_dice:0.6379487094440529
Epoch:64/120, lr:0.0005
Epoch:64，train_loss:0.8620579440253121
Epoch:64, val_loss:0.5993600406629198，val_iou:0.5509650723407805，val_dice:0.6617826769691496
=> saved best model——2025.04.11-13:15:02
Epoch:65/120, lr:0.0005
Epoch:65，train_loss:0.9051540340696063
Epoch:65, val_loss:0.6615431674701326，val_iou:0.5375796302089477，val_dice:0.6523745975600175
Epoch:66/120, lr:0.0005
Epoch:66，train_loss:0.8631431347983224
Epoch:66, val_loss:0.6648325757214836，val_iou:0.5137565258265783，val_dice:0.6346931691770495
Epoch:67/120, lr:0.0005
Epoch:67，train_loss:0.8438890716007778
Epoch:67, val_loss:0.6040542349219322，val_iou:0.549783550114267，val_dice:0.6694735079156962
Epoch:68/120, lr:0.0005
Epoch:68，train_loss:0.8483145311900547
Epoch:68, val_loss:0.6068575487417334，val_iou:0.5517469433311132，val_dice:0.6619871322326003
=> saved best model——2025.04.11-13:20:56
Epoch:69/120, lr:0.0005
Epoch:69，train_loss:0.8237103802817208
Epoch:69, val_loss:0.734816904672805，val_iou:0.4911882760764889，val_dice:0.616211736034817
Epoch:70/120, lr:0.0005
Epoch:70，train_loss:0.8529989079066685
Epoch:70, val_loss:0.6292018016033313，val_iou:0.5515248196640904，val_dice:0.6701156138891646
Epoch:71/120, lr:0.0005
Epoch:71，train_loss:0.8041421832357134
Epoch:71, val_loss:0.554567742581461，val_iou:0.5902703198933226，val_dice:0.6946485842873035
=> saved best model——2025.04.11-13:25:23
Epoch:72/120, lr:0.0005
Epoch:72，train_loss:0.8289295455387661
Epoch:72, val_loss:0.619756175241634，val_iou:0.5425267048478264，val_dice:0.6580035595430058
Epoch:73/120, lr:0.0005
Epoch:73，train_loss:0.8505378791264125
Epoch:73, val_loss:0.7395952305226934，val_iou:0.5021253049417332，val_dice:0.6198677683368209
Epoch:74/120, lr:0.0005
Epoch:74，train_loss:0.8379793671199254
Epoch:74, val_loss:0.5054641904345915，val_iou:0.6211984275564184，val_dice:0.722035224552396
=> saved best model——2025.04.11-13:29:49
Epoch:75/120, lr:0.0005
Epoch:75，train_loss:0.8077630329132081
Epoch:75, val_loss:0.6050075486886735，val_iou:0.5601069091573635，val_dice:0.6743341148335282
Epoch:76/120, lr:0.0005
Epoch:76，train_loss:0.8236308179582869
Epoch:76, val_loss:0.7120744362473488，val_iou:0.4969739795015821，val_dice:0.6140571908388994
Epoch:77/120, lr:0.0005
Epoch:77，train_loss:0.8254966609818595
Epoch:77, val_loss:0.6009815756742861，val_iou:0.5590893969715527，val_dice:0.6741737225221665
Epoch:78/120, lr:0.0005
Epoch:78，train_loss:0.8590296472821917
Epoch:78, val_loss:0.5482419093011641，val_iou:0.5860717474754684，val_dice:0.6924845895430919
Epoch:79/120, lr:0.0005
Epoch:79，train_loss:0.827349031993321
Epoch:79, val_loss:0.5420932504622376，val_iou:0.599050573549541，val_dice:0.7014422092477168
Epoch:80/120, lr:0.0005
Epoch:80，train_loss:0.8407512061936515
Epoch:80, val_loss:0.6771998141033977，val_iou:0.5456469643584217，val_dice:0.658104440479054
Epoch:81/120, lr:0.0005
Epoch:81，train_loss:0.8325394756453378
Epoch:81, val_loss:0.49509265098501654，val_iou:0.6351017131763572，val_dice:0.7323004851688901
=> saved best model——2025.04.11-13:40:10
Epoch:82/120, lr:0.0005
Epoch:82，train_loss:0.78706374168396
Epoch:82, val_loss:0.65244843606271，val_iou:0.525412114470736，val_dice:0.6512003603214755
Epoch:83/120, lr:0.0005
Epoch:83，train_loss:0.80879008940288
Epoch:83, val_loss:0.48319441087397874，val_iou:0.6334176201764085，val_dice:0.7282255025412965
Epoch:84/120, lr:0.0005
Epoch:84，train_loss:0.8113661909103393
Epoch:84, val_loss:0.6485856223632308，val_iou:0.54630926594512，val_dice:0.6647285388339303
Epoch:85/120, lr:0.0005
Epoch:85，train_loss:0.8008083295822144
Epoch:85, val_loss:0.5284905020951056，val_iou:0.5998343350990832，val_dice:0.7129032305885342
Epoch:86/120, lr:0.0005
Epoch:86，train_loss:0.7956450891494751
Epoch:86, val_loss:0.46127285508840693，val_iou:0.6507669698811885，val_dice:0.74576030274165
=> saved best model——2025.04.11-13:47:35
Epoch:87/120, lr:0.0005
Epoch:87，train_loss:0.7900160026550292
Epoch:87, val_loss:0.5795382332129806，val_iou:0.5648464414643255，val_dice:0.6727365050152042
Epoch:88/120, lr:0.0005
Epoch:88，train_loss:0.8033562844140189
Epoch:88, val_loss:0.6488902069189969，val_iou:0.5239674777816202，val_dice:0.6451467407994372
Epoch:89/120, lr:0.0005
Epoch:89，train_loss:0.7638192708151681
Epoch:89, val_loss:0.5185254711438628，val_iou:0.60255396727453，val_dice:0.7089889916865925
Epoch:90/120, lr:0.0005
Epoch:90，train_loss:0.7861981119428362
Epoch:90, val_loss:0.4677721567305864，val_iou:0.6409531460776915，val_dice:0.739374555223607
Epoch:91/120, lr:0.0005
Epoch:91，train_loss:0.7630697124344962
Epoch:91, val_loss:0.4803486570131545，val_iou:0.6298052519291719，val_dice:0.7316537614283264
Epoch:92/120, lr:0.0005
Epoch:92，train_loss:0.7776625665596554
Epoch:92, val_loss:0.46656236556522984，val_iou:0.6396599372239137，val_dice:0.7358795400976743
Epoch:93/120, lr:0.0005
Epoch:93，train_loss:0.7698738006183079
Epoch 00094: reducing learning rate of group 0 to 2.5000e-04.
Epoch:93, val_loss:0.4628677186282242，val_iou:0.6464456646599659，val_dice:0.7417597489173108
Epoch:94/120, lr:0.00025
Epoch:94，train_loss:0.7722541870389665
Epoch:94, val_loss:0.503868687605741，val_iou:0.6176212368876854，val_dice:0.7203334452059816
Epoch:95/120, lr:0.00025
Epoch:95，train_loss:0.7471770552226475
Epoch:95, val_loss:0.47349614569661663，val_iou:0.6396599710008737，val_dice:0.7338511577034658
Epoch:96/120, lr:0.00025
Epoch:96，train_loss:0.764567289352417
Epoch:96, val_loss:0.4924480747796741，val_iou:0.6313133874551508，val_dice:0.7299027401463176
Epoch:97/120, lr:0.00025
Epoch:97，train_loss:0.7377101915223258
Epoch:97, val_loss:0.5087791942790443，val_iou:0.6268641443261833，val_dice:0.7259177470389088
Epoch:98/120, lr:0.00025
Epoch:98，train_loss:0.7622570817811148
Epoch:98, val_loss:0.46372140527648087，val_iou:0.641216130723372，val_dice:0.737290004391086
Epoch:99/120, lr:0.00025
Epoch:99，train_loss:0.7764050963946751
Epoch:99, val_loss:0.4233688708467811，val_iou:0.6776483088283006，val_dice:0.7661114582146075
=> saved best model——2025.04.11-14:06:52
Epoch:100/120, lr:0.00025
Epoch:100，train_loss:0.7782316211291722
Epoch:100, val_loss:0.5029180054278934，val_iou:0.627928005217279，val_dice:0.7276424651533846
Epoch:101/120, lr:0.00025
Epoch:101，train_loss:0.741673218182155
Epoch:101, val_loss:0.5053399582119549，val_iou:0.6279332742277937，val_dice:0.7304194493687518
Epoch:102/120, lr:0.00025
Epoch:102，train_loss:0.7661819192341396
Epoch:102, val_loss:0.5127148711506058，val_iou:0.626567526632165，val_dice:0.7240468977012184
Epoch:103/120, lr:0.00025
Epoch:103，train_loss:0.7549579811096191
Epoch:103, val_loss:0.605299057548537，val_iou:0.5726939855843001，val_dice:0.6808690424710815
Epoch:104/120, lr:0.00025
Epoch:104，train_loss:0.7445682195254735
Epoch:104, val_loss:0.5119749184917001，val_iou:0.6242491044577401，val_dice:0.721044005970953
Epoch:105/120, lr:0.00025
Epoch:105，train_loss:0.728784829548427
Epoch:105, val_loss:0.4768034414771725，val_iou:0.6488139837322644，val_dice:0.7455165333359097
Epoch:106/120, lr:0.00025
Epoch:106，train_loss:0.7604688252721514
Epoch 00107: reducing learning rate of group 0 to 1.2500e-04.
Epoch:106, val_loss:0.4239768853374556，val_iou:0.6722837302645995，val_dice:0.76479343468881
Epoch:107/120, lr:0.000125
Epoch:107，train_loss:0.7520895072392055
Epoch:107, val_loss:0.47096043737495646，val_iou:0.6448825378731129，val_dice:0.7431011167995022
Epoch:108/120, lr:0.000125
Epoch:108，train_loss:0.7466012637955802
Epoch:108, val_loss:0.48288777638591973，val_iou:0.641397021030911，val_dice:0.7399954448983166
Epoch:109/120, lr:0.000125
Epoch:109，train_loss:0.7366736933163234
Epoch:109, val_loss:0.4791558352025116，val_iou:0.6392318272624737，val_dice:0.7395536781830052
Epoch:110/120, lr:0.000125
Epoch:110，train_loss:0.7413325323377337
Epoch:110, val_loss:0.48341234956009715，val_iou:0.6425445272766912，val_dice:0.740377527579864
Epoch:111/120, lr:0.000125
Epoch:111，train_loss:0.7276398263658796
Epoch:111, val_loss:0.48149296086208493，val_iou:0.6488588372286224，val_dice:0.7456213230663021
Epoch:112/120, lr:0.000125
Epoch:112，train_loss:0.7303992550713675
Epoch:112, val_loss:0.44045695238838006，val_iou:0.6651610125646474，val_dice:0.7594408993218283
Epoch:113/120, lr:0.000125
Epoch:113，train_loss:0.7074907667296273
Epoch 00114: reducing learning rate of group 0 to 6.2500e-05.
Epoch:113, val_loss:0.4478859896315079，val_iou:0.6600614402988695，val_dice:0.7526861726330685
Epoch:114/120, lr:6.25e-05
Epoch:114，train_loss:0.7291501774106707
Epoch:114, val_loss:0.44500092015254733，val_iou:0.6678536088738024，val_dice:0.7587466895206436
Epoch:115/120, lr:6.25e-05
Epoch:115，train_loss:0.694474915776934
Epoch:115, val_loss:0.47546278473501113，val_iou:0.6428666300473799，val_dice:0.7425475746330537
Epoch:116/120, lr:6.25e-05
Epoch:116，train_loss:0.708684104851314
Epoch:116, val_loss:0.4452462405258534，val_iou:0.6682107263780281，val_dice:0.758833579648668
Epoch:117/120, lr:6.25e-05
Epoch:117，train_loss:0.680230724470956
Epoch:117, val_loss:0.4351542526308228，val_iou:0.6709687519161311，val_dice:0.7616846153707002
Epoch:118/120, lr:6.25e-05
Epoch:118，train_loss:0.7201756477355957
Epoch:118, val_loss:0.45015637277095927，val_iou:0.6628204762705382，val_dice:0.7537967940032687
Epoch:119/120, lr:6.25e-05
Epoch:119，train_loss:0.7112429356575012
Epoch:119, val_loss:0.43720205442286003，val_iou:0.670986316323348，val_dice:0.7627486945931414
Training Done!  Start testing.——2025.04.11-14:36:30
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
The best model has been loaded.——2025.04.11-14:36:30
test_iou:0.7236964035059514, test_dice:0.8152087838312339.
Test Done!——2025.04.11-14:37:19
